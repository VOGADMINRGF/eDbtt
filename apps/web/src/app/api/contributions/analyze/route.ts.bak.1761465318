// apps/web/src/app/api/contributions/analyze/route.ts
import { NextRequest, NextResponse } from "next/server";
import { analyzeContribution } from "@features/analyze/analyzeContribution";
import { orchestrateContribution as analyzeMulti } from "@features/ai/orchestrator_contrib";
import { runOpenAI } from "@features/ai/providers/openai";

export const runtime = "nodejs";
export const dynamic = "force-dynamic";

function safeJson<T=any>(s:string):T|null{ try{return JSON.parse(s)}catch{return null;} }
async function recordUsageSafe(e:any){
  try{
    const m = await import("@/lib/metrics/usage");
    if (typeof (m as any).recordUsage === "function") await (m as any).recordUsage(e);
  }catch{}
}

// Harte Rückfallebene: Claims im JSON erzwingen
async function extractClaimsFallback(text:string, maxClaims=3){
  const sys = `Extrahiere bis zu ${maxClaims} prägnante, abstimmungsfähige Aussagen (Claims) als JSON:
  { "claims":[{ "text": string }]} — NUR JSON zurückgeben.`;
  const prompt = `Text:\n"""${text.slice(0,6000)}"""\nNur JSON ausgeben.`;
  const r = await runOpenAI(prompt,{ json:true, system:sys, timeoutMs:15000 });
  const json = r.ok ? safeJson<{claims?:{text:string}[]}>(r.text?.trim()||"") : null;
  const claims = Array.isArray(json?.claims) ? json!.claims.filter(c=>typeof c?.text==="string" && c.text.trim()) : [];
  return { claims, _meta:{ fallback:true, model:process.env.OPENAI_MODEL??null, tookMs:r.ms, usage:r.usage, error:r.ok?null:r.error } };
}

function forceStable(out:any, ms:number, note?:string){
  const base = { _meta:{ mode:"error", errors: note?[note]:[], tookMs:ms }, claims:[] as any[] };
  if(!out || typeof out!=="object") return base;
  if(!("_meta" in out)) return { ...base, result: out };
  if(!("claims" in out)) return { ...out, claims:[] };
  return out;
}

export async function POST(req: NextRequest){
  const t0 = Date.now();
  let ok=false, err:string|null=null, model:string|null=null, totalTokens:number|null=null;

  try{
    const u = new URL(req.url);
    const mode = u.searchParams.get("mode") || process.env.VOG_ANALYZE_MODE || "gpt";
    const body = await req.json().catch(()=>({}));
    const text = String(body?.text ?? "").trim().slice(0,8000);
    const maxClaims = Number(body?.maxClaims ?? 3);

    if(!text){
      const ms = Date.now()-t0;
      return NextResponse.json(forceStable(null, ms, "no-text"), { status:200 });
    }

    if(mode==="multi"){
      const orches = await analyzeMulti(text,{ maxClaims }).catch(()=>null);
      const bestText = String(orches?.best?.text ?? text);

      let extracted = await analyzeContribution(bestText,{ maxClaims }).catch(()=>({claims:[], _meta:{} as any}));
      if(!Array.isArray(extracted?.claims) || extracted.claims.length===0){
        const fb = await extractClaimsFallback(bestText, maxClaims);
        extracted = { ...(extracted||{}), claims: fb.claims, _meta: { ...(extracted?._meta??{}), fallbackUsed:true } };
      }

      extracted._meta = { ...(extracted._meta??{}), mode:"multi+extract", tookMs: Date.now()-t0, provider: orches?.best?.provider ?? null };
      model = (extracted?._meta?.model ?? process.env.OPENAI_MODEL ?? null) as any;
      totalTokens = (extracted?._meta?.usage?.total_tokens ?? null) as any;
      ok=true;
      return NextResponse.json(forceStable(extracted, extracted._meta.tookMs), { status:200 });
    }

    // Standard
    let out = await analyzeContribution(text,{ maxClaims }).catch(()=>({claims:[], _meta:{} as any}));
    if(!Array.isArray(out?.claims) || out.claims.length===0){
      const fb = await extractClaimsFallback(text, maxClaims);
      out = { ...(out||{}), claims: fb.claims, _meta: { ...(out?._meta??{}), fallbackUsed:true } };
    }

    out._meta = { ...(out._meta??{}), mode:"gpt", tookMs: Date.now()-t0 };
    model = (out?._meta?.model ?? process.env.OPENAI_MODEL ?? null) as any;
    totalTokens = (out?._meta?.usage?.total_tokens ?? null) as any;
    ok=true;
    return NextResponse.json(forceStable(out, out._meta.tookMs), { status:200 });

  }catch(e:any){
    err = String(e?.message || e);
    const ms = Date.now()-t0;
    return NextResponse.json(forceStable(null, ms, err), { status:200 });

  }finally{
    await recordUsageSafe({ ts:Date.now(), route:"/api/contributions/analyze", userId:null, model, totalTokens, ms: Date.now()-t0, ok, err, meta:{ source:"handoff" } });
  }
}
