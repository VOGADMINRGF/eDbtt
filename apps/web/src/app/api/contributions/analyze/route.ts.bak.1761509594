import { NextRequest, NextResponse } from "next/server";
import { recordUsage } from "@/lib/metrics/usage";

// optionaler Analyzer aus deinem features-Paket (wenn vorhanden)
let analyzeContribution: any = null;
try{ analyzeContribution = (await import("@/features/analyze/analyzeContribution")).analyzeContribution; }catch{}
let orchestrateContribution: any = null;
try{ orchestrateContribution = (await import("@/features/ai/orchestrator_contrib")).orchestrateContribution; }catch{}

// Fallback-Runner (Responses API)
import { runOpenAI } from "@/features/ai/providers/openai";

export const runtime = "nodejs";
export const dynamic = "force-dynamic";

function safeJson<T=any>(s:string): T|null { try{ return JSON.parse(s) as T; }catch{ return null; } }

async function extractClaimsFallback(text: string, maxClaims=3){
  const sys = `Extrahiere bis zu ${maxClaims} prägnante, abstimm-bare Claims als JSON.
Jeder Claim kurz, eindeutig, neutral formulierbar. 
Antwortformat STRICT:
{ "claims": [ { "text": string } ] }`;
  const prompt = `Text:
\"\"\"${text.slice(0,6000)}\"\"\"\n\nGib NUR das JSON-Objekt zurück.`;
  const r = await runOpenAI(prompt, { json: true, system: sys, timeoutMs: 15000 });
  if(!r.ok) return { claims: [], _meta:{ fallback:true, error:r.error ?? null, ms:r.ms } };
  const j = safeJson<{claims?: Array<{text:string}>}>(r.text.trim());
  const claims = Array.isArray(j?.claims) ? j!.claims.filter(c=>typeof c?.text==="string" && c.text.trim()) : [];
  return { claims, _meta:{ fallback:true, ms:r.ms, usage:r.usage, model: process.env.OPENAI_MODEL ?? null } };
}

function forceStable(out:any, ms:number, note?:string){
  const base = { _meta:{ mode:"error", errors: note? [note] : [], tookMs: ms }, claims: [] as any[] };
  if(!out || typeof out!=="object") return base;
  if(!("_meta" in out)) return { ...base, result: out };
  if(!("claims" in out)) return { ...out, claims: [] };
  return out;
}
function parseClarifications(raw: string){
  const grab = (label: string) => {
    const re = new RegExp(`${label}\\s*:\\s*([^\\n]+)`, "i");
    const m = raw.match(re);
    return m ? m[1].trim() : null;
  };
  return {
    ebene: grab("Ebene"),
    ort: grab("Ort"),
    zeitraum: grab("Zeitraum"),
    betroffene: grab("Betroffene"),
    haltung: grab("Haltung"),
  };
}

function buildSystemAddon(c:any){
  const parts:string[] = [];
  if (c.ebene)     parts.push(`Politische Ebene/Zuständigkeit: ${c.ebene}`);
  if (c.ort)       parts.push(`Ort/Region: ${c.ort}`);
  if (c.zeitraum)  parts.push(`Zeitraum: ${c.zeitraum}`);
  if (c.betroffene)parts.push(`Betroffene: ${c.betroffene}`);
  if (c.haltung)   parts.push(`Haltung (Selbsteinschätzung): ${c.haltung}`);
  return parts.length ? `\n[Kontext-Präzisierung]\n${parts.join("\n")}\n` : "";
}

export async function POST(req: NextRequest){
  const t0 = Date.now();
  let ok=false, err:string|null=null, model:string|null=null, totalTokens:number|null=null;

  try{
    const u = new URL(req.url);
    const mode = u.searchParams.get("mode") || process.env.VOG_ANALYZE_MODE || "gpt";
    const body = await req.json().catch(()=> ({}));
    const text = String(body?.text ?? "").trim().slice(0,8000);
    const maxClaims = Number(body?.maxClaims ?? 3);
    const clar = parseClarifications(text);
    const systemAddon = buildSystemAddon(clar);


    if(!text){
      const ms=Date.now()-t0; ok=true;
      return NextResponse.json(forceStable(null, ms, "no-text"), { status:200 });
    }

    // MULTI: orchestrator + extract (falls verfügbar)
    if(mode==="multi" && orchestrateContribution){
      const orches = await orchestrateContribution(text, { maxClaims }).catch(()=>null);
      const bestText = String(orches?.best?.text ?? text);

      let out = analyzeContribution
        ? await analyzeContribution(bestText, { maxClaims }).catch(()=> ({ claims:[], _meta:{} as any }))
        : { claims:[], _meta:{} as any };

      if(!Array.isArray(out?.claims) || out.claims.length===0){
        const fb = await extractClaimsFallback(bestText, maxClaims);
        out = { ...(out||{}), claims: fb.claims, _meta: { ...(out?._meta??{}), fallbackUsed:true } };
      }

      out._meta = { ...(out._meta??{}), mode: "multi+extract", tookMs: Date.now()-t0, provider: orches?.best?.provider ?? null };
      model = (out?._meta?.model ?? process.env.OPENAI_MODEL ?? null) as any;
      totalTokens = (out?._meta?.usage?.total_tokens ?? null) as any;
      ok=true;
      return NextResponse.json(forceStable(out, out._meta.tookMs), { status:200 });
    }

    // Standard: direkte Claim-Extraktion (präferiere vorhandenen Analyzer)
    let out = analyzeContribution
      ? await analyzeContribution(text, { maxClaims }).catch(()=> ({ claims:[], _meta:{} as any }))
      : { claims:[], _meta:{} as any };

    if(!Array.isArray(out?.claims) || out.claims.length===0){
      const fb = await extractClaimsFallback(text, maxClaims);
      out = { ...(out||{}), claims: fb.claims, _meta: { ...(out?._meta??{}), fallbackUsed:true } };
    }

    out._meta = { ...(out._meta??{}), mode:"gpt", tookMs: Date.now()-t0 };
    model = (out?._meta?.model ?? process.env.OPENAI_MODEL ?? null) as any;
    totalTokens = (out?._meta?.usage?.total_tokens ?? null) as any;
    ok=true;
    return NextResponse.json(forceStable(out, out._meta.tookMs), { status:200 });

  }catch(e:any){
    err = String(e?.message || e);
    const ms = Date.now()-t0;
    return NextResponse.json(forceStable(null, ms, err), { status:200 });
  }finally{
    await recordUsage({
      ts: Date.now(),
      route: "/api/contributions/analyze",
      userId: null,
      model, totalTokens,
      ms: Date.now()-t0, ok, err,
      meta: { source: "super_bundle" }
    });
  }
}
