import { NextRequest, NextResponse } from "next/server";
import { analyzeContribution } from "@/features/analyze/analyzeContribution";
import { orchestrateContribution as analyzeMulti } from "@/features/ai/orchestrator_contrib";
import { runOpenAI } from "@/features/ai/providers/openai";

export const runtime = "nodejs";
export const dynamic = "force-dynamic";

function safeJson<T=any>(s:string){ try{ return JSON.parse(s) as T; }catch{ return null; } }
async function recordUsageSafe(e:any){ try{ const m=await import("@/lib/metrics/usage"); const fn=(m as any)?.recordUsage; if(typeof fn==="function") await fn(e);}catch{} }

async function extractClaimsFallback(text:string, maxClaims=3){
  const sys = `Extrahiere bis zu ${maxClaims} prägnante Claims als JSON: {"claims":[{"text":string}]}`;
  const prompt = `Text:\n"""${text.slice(0,6000)}"""\nNur JSON.`;
  const r = await runOpenAI(prompt, { json:true, system: sys, timeoutMs: 12000 });
  const json = r.ok ? safeJson<{claims?:Array<{text:string}>}>(r.text?.trim()||"") : null;
  const claims = Array.isArray(json?.claims) ? json!.claims.filter(c=>c?.text && c.text.trim()).slice(0,maxClaims) : [];
  return { claims, _meta:{ fallback:true, model: process.env.OPENAI_MODEL ?? null, tookMs: r.ms, usage: r.usage, error: r.ok? null: r.error } };
}

function forceStable(out:any, ms:number, note?:string){
  const base = { _meta:{ mode:"error", errors: note? [note]:[], tookMs: ms }, claims: [] as any[] };
  if(!out || typeof out!=="object") return base;
  if(!("_meta" in out)) return { ...base, result: out };
  if(!("claims" in out)) return { ...out, claims: [] };
  return out;
}

function normalizeHints(h:any){
  // Erlaube Arrays; primär erster Eintrag; komplette Auswahl im „other“
  const n = (v:any)=> Array.isArray(v)? v : (v==null? []:[v]);
  const out:any = {};
  const levels = n(h?.level); const regions = n(h?.region); const tfs = n(h?.timeframe); const aud = n(h?.audience); const st = n(h?.stance);
  out.level   = levels[0]   || "unsicher";
  out.region  = regions[0]  || null;
  out.timeframe = tfs[0]    || "unsicher";
  out.audience  = aud[0]    || "unsicher";
  out.stance    = st[0]     || "unsicher";
  out.other = { levels, regions, timeframes: tfs, audiences: aud, stances: st, ...(h?.other||{}) };
  return out;
}

export async function POST(req: NextRequest){
  const t0=Date.now(); let ok=false, err:string|null=null, model:string|null=null, totalTokens:number|null=null;

  try{
    const u=new URL(req.url);
    const mode = u.searchParams.get("mode") || process.env.VOG_ANALYZE_MODE || "gpt";
    const deadlineMs = Number(u.searchParams.get("deadlineMs") || 0) || undefined;
    const hardExtractMs = Number(process.env.VOG_EXTRACT_DEADLINE_MS || 10000); // 10s Kappe

    const body = await req.json().catch(()=> ({}));
    const text = String(body?.text ?? "").trim().slice(0,8000);
    const maxClaims = Math.max(1, Number(body?.maxClaims ?? 3));
    const hintsRaw  = (typeof body?.hints === "object" && body.hints) ? body.hints : null;
    const hints     = hintsRaw ? normalizeHints(hintsRaw) : null;

    if(!text){
      const ms=Date.now()-t0;
      ok=true; return NextResponse.json(forceStable(null, ms, "no-text"), {status:200});
    }

    // —— MULTI: Orchestrator (parallel, deadline) —— //
    if(mode==="multi"){
      const orches = await analyzeMulti(text, { maxClaims, deadlineMs: deadlineMs ?? 22000 }).catch(()=> null);
      const bestText = String(orches?.best?.text ?? text);

      // Claim-Extraktion hart deckeln; bei Timeout -> Fallback
      let extracted:any = null; let timedOut=false;
      try{
        extracted = await Promise.race([
          analyzeContribution(bestText, { maxClaims, hints }),
          new Promise(res=> setTimeout(()=> { timedOut=true; res("__TIMEOUT__"); }, hardExtractMs))
        ]);
      }catch{}

      if(extracted === "__TIMEOUT__" || !extracted){
        const fb = await extractClaimsFallback(bestText, maxClaims);
        extracted = { claims: fb.claims, _meta: { fallbackUsed:true, ...(fb as any)._meta } };
      }

      // Absicherung: leere Ergebnisse -> nochmals Fallback
      if(!Array.isArray(extracted?.claims) || extracted.claims.length===0){
        const fb = await extractClaimsFallback(bestText, maxClaims);
        extracted = { ...(extracted||{}), claims: fb.claims, _meta:{ ...(extracted?._meta??{}), fallbackUsed:true } };
      }

      // Dedup & trim
      const uniq = Array.from(new Set(extracted.claims.map((c:any)=> String(c?.text||"").trim()).filter(Boolean)))
                        .slice(0, maxClaims)
                        .map(t=> ({ text: t }));

      extracted.claims = uniq.length? uniq : [{ text: bestText.slice(0,280) }];

      extracted._meta = { ...(extracted._meta??{}), mode:"multi+extract", tookMs: Date.now()-t0, provider: orches?.best?.provider ?? null };
      model = (extracted?._meta?.model ?? process.env.OPENAI_MODEL ?? null) as any;
      totalTokens = (extracted?._meta?.usage?.total_tokens ?? null) as any;
      ok=true;
      return NextResponse.json(forceStable(extracted, extracted._meta.tookMs), {status:200});
    }

    // —— STANDARD —— //
    let out:any = null; let timed=false;
    try{
      out = await Promise.race([
        analyzeContribution(text, { maxClaims, hints }),
        new Promise(res=> setTimeout(()=> { timed=true; res("__TIMEOUT__"); }, hardExtractMs))
      ]);
    }catch{}
    if(out === "__TIMEOUT__" || !out || !Array.isArray(out?.claims) || out.claims.length===0){
      const fb = await extractClaimsFallback(text, maxClaims);
      out = { claims: fb.claims, _meta:{ ...(out?._meta??{}), fallbackUsed:true } };
    }
    out._meta = { ...(out._meta??{}), mode:"gpt", tookMs: Date.now()-t0 };
    model = (out?._meta?.model ?? process.env.OPENAI_MODEL ?? null) as any;
    totalTokens = (out?._meta?.usage?.total_tokens ?? null) as any;
    ok=true;
    return NextResponse.json(forceStable(out, out._meta.tookMs), {status:200});
  }catch(e:any){
    err=String(e?.message||e);
    const ms=Date.now()-t0;
    return NextResponse.json(forceStable(null, ms, err), {status:200});
  }finally{
    await recordUsageSafe({ ts:Date.now(), route:"/api/contributions/analyze", userId:null, model, totalTokens, ms: Date.now()-t0, ok, err, meta:{ source:"perf-hotfix" } });
  }
}
