import { NextRequest, NextResponse } from "next/server";
import { analyzeContribution } from "@/features/analyze/analyzeContribution";
import { orchestrateContribution as analyzeMulti } from "@/features/ai/orchestrator_contrib";
import { runOpenAI } from "@/features/ai/providers/openai";

export const runtime = "nodejs";
export const dynamic = "force-dynamic";

function safeJson<T=any>(s:string):T|null { try{return JSON.parse(s) as T;}catch{return null;} }

async function recordUsageSafe(e:any){
  try{ const m = await import("@/lib/metrics/usage"); const fn=(m as any)?.recordUsage;
       if (typeof fn==="function") await fn(e);
  }catch{}
}

async function extractClaimsFallback(text:string, maxClaims=3, hints?:any){
  const scopeLines:string[] = [];
  if (hints?.level) scopeLines.push(`Zuständigkeit (Hinweis): ${hints.level}`);
  if (hints?.region) scopeLines.push(`Region (Hinweis): ${hints.region}`);
  if (hints?.timeframe) scopeLines.push(`Zeitraum (Hinweis): ${hints.timeframe}`);
  const scope = scopeLines.length ? `Berücksichtige diese optionalen Hinweise:\n- ${scopeLines.join("\n- ")}\n` : "";

  const sys = `Extrahiere bis zu ${maxClaims} prägnante Claims in JSON.
Bevorzuge ausformulierte, prüfbare Sätze. ${scope}
Antwortformat STRICT:
{ "claims": [ { "text": string } ] }`;
  const prompt = `Text:
"""${text.slice(0,6000)}"""
Gib NUR das JSON-Objekt zurück.`;

  const r = await runOpenAI(prompt, { json:true, system: sys, timeoutMs: 16000 });
  if (!r.ok) return { claims: [], _meta:{ fallback:true, error:r.error??null } };

  const json = safeJson<{claims?:Array<{text:string}>}>(r.text?.trim()||"");
  const claims = Array.isArray(json?.claims) ? json!.claims.filter(c => typeof c?.text==="string" && c.text.trim()) : [];
  return { claims, _meta:{ fallback:true, model: process.env.OPENAI_MODEL??null, tookMs: r.ms, usage: r.usage } };
}

function forceStable(out:any, ms:number, note?:string){
  const base = { _meta:{ mode:"error", errors: note ? [note] : [], tookMs: ms }, claims: [] as any[] };
  if (!out || typeof out!=="object") return base;
  if (!("_meta" in out)) return { ...base, result: out };
  if (!("claims" in out)) return { ...out, claims: [] };
  return out;
}

export async function POST(req: NextRequest){
  const t0 = Date.now();
  let ok=false, err:string|null=null, model:string|null=null, totalTokens:number|null=null;

  try{
    const u = new URL(req.url);
    const mode  = u.searchParams.get("mode") || process.env.VOG_ANALYZE_MODE || "gpt";
    const body  = await req.json().catch(()=> ({}));
    const text  = String(body?.text ?? "").trim().slice(0, 8000);
    const maxClaims = Number(body?.maxClaims ?? 3);
    const hints = (body?.hints && typeof body.hints==="object") ? body.hints : undefined;

    if (!text) {
      const ms = Date.now()-t0;
      const payload = forceStable(null, ms, "no-text");
      ok = true;
      return NextResponse.json(payload, { status: 200 });
    }

    if (mode === "multi") {
      const orches = await analyzeMulti(text, { maxClaims }).catch(()=>null);
      const bestText = String(orches?.best?.text ?? text);

      let extracted = await analyzeContribution(bestText, {
        maxClaims,
        context: { editorSignals: { hints } },
      }).catch(()=>({ claims:[], _meta:{} as any }));

      if (!Array.isArray(extracted?.claims) || extracted.claims.length===0) {
        const fb = await extractClaimsFallback(bestText, maxClaims, hints);
        extracted = { ...(extracted||{}), claims: fb.claims, _meta: { ...(extracted?._meta??{}), fallbackUsed:true } };
      }

      extracted._meta = {
        ...(extracted._meta??{}),
        mode: "multi+extract",
        tookMs: Date.now()-t0,
        provider: orches?.best?.provider ?? null,
      };

      model       = (extracted?._meta?.model ?? process.env.OPENAI_MODEL ?? null) as any;
      totalTokens = (extracted?._meta?.usage?.total_tokens ?? null) as any;
      ok = true;
      return NextResponse.json(forceStable(extracted, extracted._meta.tookMs), { status: 200 });
    }

    // Standard: direkte Claim-Extraktion
    let out = await analyzeContribution(text, {
      maxClaims,
      context: { editorSignals: { hints } },
    }).catch(()=>({ claims:[], _meta:{} as any }));

    if (!Array.isArray(out?.claims) || out.claims.length===0) {
      const fb = await extractClaimsFallback(text, maxClaims, hints);
      out = { ...(out||{}), claims: fb.claims, _meta:{ ...(out?._meta??{}), fallbackUsed:true } };
    }

    out._meta = { ...(out._meta??{}), mode:"gpt", tookMs: Date.now()-t0 };
    model       = (out?._meta?.model ?? process.env.OPENAI_MODEL ?? null) as any;
    totalTokens = (out?._meta?.usage?.total_tokens ?? null) as any;
    ok = true;
    return NextResponse.json(forceStable(out, out._meta.tookMs), { status: 200 });

  }catch(e:any){
    err = String(e?.message||e);
    const ms = Date.now()-t0;
    const payload = forceStable(null, ms, err);
    return NextResponse.json(payload, { status: 200 });

  }finally{
    await recordUsageSafe({
      ts: Date.now(),
      route: "/api/contributions/analyze",
      userId: null, model, totalTokens,
      ms: Date.now()-t0, ok, err,
      meta: { source: "handoff" }
    });
  }
}
